{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interacting with CLIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Interacting with CLIP\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uduc89fCSDrZ"
      },
      "source": [
        "import os\n",
        "\n",
        "def get_free_path(path):\n",
        "  i = 0\n",
        "  \n",
        "  while os.path.exists(path):\n",
        "      i += 1\n",
        "      path = path + (\"_%02d\" % i)\n",
        "  return path"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKGXPRWIlKqy"
      },
      "source": [
        "\n",
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr5IgopOaQFM",
        "outputId": "4c392c7a-e107-4052-aad0-6ec27cdf52b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ipfs_path = \"/content/ipfs\"\n",
        "\n",
        "experiment_name = \"posthuman_final\"\n",
        "\n",
        "experiment_path_suffix = f\"video_to_clip/{experiment_name}\" #get_free_path(f\"{ipfs_path}/video_to_clip/{experiment_name}\")\n",
        "experiment_path = f\"{ipfs_path}/{experiment_path_suffix}\"\n",
        "\n",
        "results_path = f\"{experiment_path}/results\"\n",
        "\n",
        "top_classes=5\n",
        "\n",
        "# The words only take effect if cifar is False (otherwise the 100 CIFAR categories will be used)\n",
        "cifar = False\n",
        "#words = [\"Happiness\", \"Excitement\", \"Boredom\", \"Sadness\",\"Anger\", \"Disgust\", \"Fear\"] \n",
        "#words = [\"baby\",\"bed\",\"bicycle\",\"bottle\",\"bowl\",\"boy\",\"bridge\",\"bus\",\"can\",\"castle\",\"chair\",\"clock\",\"cloud\",\"couch\",\"cup\",\"girl\",\"house\",\"keyboard\",\"lamp\",\"man\",\"motorcycle\",\"mountain\",\"pickup_truck\",\"plate\",\"road\",\"rocket\",\"skyscraper\",\"streetcar\",\"table\",\"tank\",\"telephone\",\"television\",\"tractor\",\"train\",\"wardrobe\",\"woman\"]\n",
        "#words = [\"compassion\",\"friendship\",\"dreams\",\"bittersweet\",\"calm\",\"relaxing\",\"heroic\",\"energizing\",\"romantic\",\"love\",\"transcendental\",\"mystical\",\"awe-inspiring\",\"eerie\",\"mysterious\",\"joyful\",\"erotic\",\"euphoric\",\"ecstatic\",\"defiant\",\"proud\",\"strong\",\"sad\",\"depressing\",\"tender\", \"magical\", \"energetic\"]\n",
        "#words = [\"shows compassion\",\"shows friendship\",\"is dreamy\",\"is calm\",\"is relaxing\",\"shows courage\", \"is energetic\",\"shows romance\",\"shows love\",\"is transcendental\",\"is mystical\",\"is awe-inspring\",\"is eerie\",\"is joyful\",\"depicts confusion\",\"is euphoric\",\"is ecstatic\",\"shows defiance\",\"shows pride\",\"shows strength\",\"is sad\",\"shows sadness\",\"shows depression\",\"is depressing\", \"is magical\", \"shows movement\",\"shows tenderness\"]\n",
        "#words = [\"woodcock\",\"duck\",\"bear\",\"river crab\",\"beaver\"] #,\"badger\",\"eagle\",\"wild boar\",\"butterfly\",\"ant\",\"spider\"]\n",
        "words = [\"woodcock\",\"duck\",\"insect\",\"mammal\",\"plant\"] #,\"badger\",\"eagle\",\"wild boar\",\"butterfly\",\"ant\",\"spider\"]\n",
        "\n",
        "print(\"Experiment Path:\",experiment_path)\n",
        "!mkdir -p $experiment_path"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment Path: /content/ipfs/video_to_clip/posthuman_final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az5qVO1mSg3p",
        "outputId": "92553c1f-0ac1-4d4d-da47-38838c192dad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"_%02d\" % 2)\n",
        "!pip install youtube-dl\n",
        "!youtube-dl https://www.youtube.com/watch?v=4hQh3G1dVpk --output $experiment_path/heaven\n",
        "!mkdir -p $experiment_path/frames\n",
        "!ffmpeg -i $experiment_path/heaven.mkv -y -vf \"crop=in_h*1/1:in_h,scale=-2:512,fps=15\" $experiment_path/frames/frame_%04d.png\n",
        "#ffmpeg -i SomeInput.mp4 -t 4 SomeOutput.mp4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_02\n",
            "Collecting youtube-dl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/47/a4442e3bd6f13013c0c38a5b16576e9d69da14d09b1ef00a9c0915e75b3e/youtube_dl-2021.5.16-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 2.9MB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25h/bin/bash: youtube-dl: command not found\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[1;31m/content/ipfs/video_to_clip/posthuman_tiergarten_4/heaven.mkv: No such file or directory\n",
            "\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjL-H3C22O6n",
        "outputId": "4066456b-10f5-4658-e99e-5d0a59f979f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://ipfs.pollinations.ai:8080/ipfs/QmdvoCnPjubRWDHZp2EbWUzmdBjZwFKhDCZrYPVL9Kfwwo -O $experiment_path/posthuman_all.mp4\n",
        "!mkdir -p $experiment_path/frames\n",
        "!ipfs get QmdvoCnPjubRWDHZp2EbWUzmdBjZwFKhDCZrYPVL9Kfwwo\n",
        "!ffmpeg -i $experiment_path/posthuman_all.mp4 -y -vf \"crop=in_h*1/1:in_h,scale=-2:512,fps=10\" $experiment_path/frames/frame_%04d.png\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-23 06:13:58--  http://ipfs.pollinations.ai:8080/ipfs/QmdvoCnPjubRWDHZp2EbWUzmdBjZwFKhDCZrYPVL9Kfwwo\n",
            "Resolving ipfs.pollinations.ai (ipfs.pollinations.ai)... 18.157.205.205\n",
            "Connecting to ipfs.pollinations.ai (ipfs.pollinations.ai)|18.157.205.205|:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 181156371 (173M) [video/mp4]\n",
            "Saving to: ‘/content/ipfs/video_to_clip/posthuman_final/posthuman_all.mp4’\n",
            "\n",
            "/content/ipfs/video 100%[===================>] 172.76M  11.3MB/s    in 17s     \n",
            "\n",
            "2021-05-23 06:14:16 (10.1 MB/s) - ‘/content/ipfs/video_to_clip/posthuman_final/posthuman_all.mp4’ saved [181156371/181156371]\n",
            "\n",
            "/bin/bash: ipfs: command not found\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/ipfs/video_to_clip/posthuman_final/posthuman_all.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 1\n",
            "    compatible_brands: isommp41mp42\n",
            "    creation_time   : 2021-05-21T21:31:24.000000Z\n",
            "  Duration: 00:07:20.00, start: -0.001451, bitrate: 3293 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc, bt709, progressive), 1280x720, 3134 kb/s, 24 fps, 24 tbr, 600 tbn, 1200 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-05-21T21:31:24.000000Z\n",
            "      handler_name    : Core Media Video\n",
            "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 154 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-05-21T21:31:24.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> png (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x557caaa44000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0m\u001b[1;34m[swscaler @ 0x557caaa44000] \u001b[0m\u001b[0;33mWarning: data is not aligned! This can lead to a speed loss\n",
            "\u001b[0mOutput #0, image2, to '/content/ipfs/video_to_clip/posthuman_final/frames/frame_%04d.png':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 1\n",
            "    compatible_brands: isommp41mp42\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: png, rgb24, 512x512, q=2-31, 200 kb/s, 10 fps, 10 tbn, 10 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-05-21T21:31:24.000000Z\n",
            "      handler_name    : Core Media Video\n",
            "      encoder         : Lavc57.107.100 png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will print the CUDA version of the runtime if it has a GPU, and install PyTorch 1.7.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT0fJnj-Hv3q",
        "outputId": "718cb5cd-d79e-46c7-93e2-e1b6a8040cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir -p $results_path\n",
        "%cd /content\n",
        "!git clone https://github.com/voodoohop/pollinations.git\n",
        "%cd /content/pollinations/app\n",
        "!git pull\n",
        "!git checkout dev\n",
        "!git pull\n",
        "!npm install\n",
        "!npm install -g\n",
        "%cd -\n",
        "#!npm install forever -g"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'pollinations'...\n",
            "remote: Enumerating objects: 1803, done.\u001b[K\n",
            "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 1803 (delta 123), reused 95 (delta 53), pack-reused 1608\u001b[K\n",
            "Receiving objects: 100% (1803/1803), 15.69 MiB | 21.42 MiB/s, done.\n",
            "Resolving deltas: 100% (1166/1166), done.\n",
            "/content/pollinations/app\n",
            "Already up to date.\n",
            "Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
            "Switched to a new branch 'dev'\n",
            "Already up to date.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m babel-eslint@10.1.0: babel-eslint is now @babel/eslint-parser. This package will no longer receive updates.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m core-js@2.6.12: core-js@<3.3 is no longer maintained and not recommended for usage due to the number of issues. Because of the V8 engine whims, feature detection in old core-js versions could cause a slowdown up to 100x even if nothing is polyfilled. Please, upgrade your dependencies to the actual version of core-js.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m chokidar@2.1.8: Chokidar 2 will break on node v14+. Upgrade to chokidar 3 with 15x less dependencies.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m text-encoding@0.7.0: no longer maintained\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m rollup-plugin-babel@4.4.0: This package has been deprecated and is no longer maintained. Please use @rollup/plugin-babel.\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @hapi/joi@15.1.1: Switch to 'npm install joi'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m urix@0.1.0: Please see https://github.com/lydell/urix#deprecated\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @hapi/topo@3.1.6: This version has been deprecated and is no longer supported or maintained\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @hapi/address@2.1.4: Moved to 'npm install @sideway/address'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @hapi/bourne@1.3.2: This version has been deprecated and is no longer supported or maintained\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m @hapi/hoek@8.5.1: This version has been deprecated and is no longer supported or maintained\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m resolve-url@0.2.1: https://github.com/lydell/resolve-url#deprecated\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m request-promise-native@1.0.9: request-promise-native has been deprecated because it extends the now deprecated request package, see https://github.com/request/request/issues/3142\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m har-validator@5.1.5: this library is no longer supported\n",
            "\u001b[K\u001b[?25h\n",
            "> iso-constants@0.1.2 install /content/pollinations/app/node_modules/iso-constants\n",
            "> node build.js > index.browser.js\n",
            "\n",
            "\u001b[K\u001b[?25h\n",
            "> core-js@2.6.12 postinstall /content/pollinations/app/node_modules/core-js\n",
            "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
            "\n",
            "\u001b[96mThank you for using core-js (\u001b[94m https://github.com/zloirock/core-js \u001b[96m) for polyfilling JavaScript standard library!\u001b[0m\n",
            "\n",
            "\u001b[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://opencollective.com/core-js \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://www.patreon.com/zloirock \u001b[0m\n",
            "\n",
            "\u001b[96mAlso, the author of core-js (\u001b[94m https://github.com/zloirock \u001b[96m) is looking for a good job -)\u001b[0m\n",
            "\n",
            "\n",
            "> core-js-pure@3.12.1 postinstall /content/pollinations/app/node_modules/core-js-pure\n",
            "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
            "\n",
            "\u001b[96mThank you for using core-js (\u001b[94m https://github.com/zloirock/core-js \u001b[96m) for polyfilling JavaScript standard library!\u001b[0m\n",
            "\n",
            "\u001b[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://opencollective.com/core-js \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://www.patreon.com/zloirock \u001b[0m\n",
            "\n",
            "\u001b[96mAlso, the author of core-js (\u001b[94m https://github.com/zloirock \u001b[96m) is looking for a good job -)\u001b[0m\n",
            "\n",
            "\n",
            "> ejs@2.7.4 postinstall /content/pollinations/app/node_modules/ejs\n",
            "> node ./postinstall.js\n",
            "\n",
            "Thank you for installing \u001b[35mEJS\u001b[0m: built with the \u001b[32mJake\u001b[0m JavaScript build tool (\u001b[32mhttps://jakejs.com/\u001b[0m)\n",
            "\n",
            "\u001b[K\u001b[?25h\n",
            "> protobufjs@6.11.2 postinstall /content/pollinations/app/node_modules/protobufjs\n",
            "> node scripts/postinstall\n",
            "\n",
            "\n",
            "> core-js@3.12.1 postinstall /content/pollinations/app/node_modules/react-app-polyfill/node_modules/core-js\n",
            "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
            "\n",
            "\u001b[96mThank you for using core-js (\u001b[94m https://github.com/zloirock/core-js \u001b[96m) for polyfilling JavaScript standard library!\u001b[0m\n",
            "\n",
            "\u001b[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://opencollective.com/core-js \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://www.patreon.com/zloirock \u001b[0m\n",
            "\n",
            "\u001b[96mAlso, the author of core-js (\u001b[94m https://github.com/zloirock \u001b[96m) is looking for a good job -)\u001b[0m\n",
            "\n",
            "\n",
            "> core-js@3.12.1 postinstall /content/pollinations/app/node_modules/react-scripts/node_modules/core-js\n",
            "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
            "\n",
            "\u001b[96mThank you for using core-js (\u001b[94m https://github.com/zloirock/core-js \u001b[96m) for polyfilling JavaScript standard library!\u001b[0m\n",
            "\n",
            "\u001b[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://opencollective.com/core-js \u001b[0m\n",
            "\u001b[96m>\u001b[94m https://www.patreon.com/zloirock \u001b[0m\n",
            "\n",
            "\u001b[96mAlso, the author of core-js (\u001b[94m https://github.com/zloirock \u001b[96m) is looking for a good job -)\u001b[0m\n",
            "\n",
            "\u001b[K\u001b[?25h\n",
            "> esbuild@0.11.23 postinstall /content/pollinations/app/node_modules/esbuild\n",
            "> node install.js\n",
            "\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@~2.3.1 (node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Unsupported engine for ipfs-unixfs@4.0.3: wanted: {\"node\":\">=14.0.0\",\"npm\":\">=7.0.0\"} (current: {\"node\":\"14.16.0\",\"npm\":\"6.14.8\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Not compatible with your version of node/npm: ipfs-unixfs@4.0.3\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.2.7 (node_modules/watchpack-chokidar2/node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.2.7 (node_modules/webpack-dev-server/node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@7.13.12 requires a peer of @babel/core@^7.13.0 but none is installed. You must install peer dependencies yourself.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m tsutils@3.21.0 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m pollinations@0.4.5 No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m pollinations@0.4.5 No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m pollinations@0.4.5 No license field.\n",
            "\u001b[0m\n",
            "added 2154 packages from 1191 contributors and audited 2164 packages in 60.607s\n",
            "\n",
            "147 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 79 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mdeprecated\u001b[0m fsevents@1.2.13: fsevents 1 will break on node v14+ and could be using insecure binaries. Upgrade to fsevents 2.\n",
            "\u001b[K\u001b[?25h/tools/node/bin/pollinate -> /tools/node/lib/node_modules/pollinations/src/backend/ipfsWatch.js\n",
            "\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@~2.3.1 (node_modules/pollinations/node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.2.7 (node_modules/pollinations/node_modules/watchpack-chokidar2/node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.2.7 (node_modules/pollinations/node_modules/webpack-dev-server/node_modules/chokidar/node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@7.13.12 requires a peer of @babel/core@^7.13.0 but none is installed. You must install peer dependencies yourself.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m tsutils@3.21.0 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.\n",
            "\u001b[0m\n",
            "+ pollinations@0.4.5\n",
            "added 1 package in 4.512s\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DnwTRLk21Wy"
      },
      "source": [
        "IPFS_ROOT = \"/content/ipfs\"\n",
        "DATA_PATH = experiment_path+\"/frames\"\n",
        "!echo \"Num input images:\" `ls -l {DATA_PATH}/*.jpg {DATA_PATH}/*.png 2>/dev/null | wc -l`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhmywm0OQ-7_"
      },
      "source": [
        "DISABLED\n",
        "\n",
        "\n",
        "```\n",
        "!apt-get install imagemagick\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook\n",
        "for file in tqdm_notebook(glob(DATA_PATH+\"/*.png\")):\n",
        "  #print(file)\n",
        "  !mogrify -resize 512 $file \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICGYGCbeNr3g"
      },
      "source": [
        "#!DEBUG=* pollinate -p /content/ipfs/video_to_clip/pixtunes_3_drive_run -s --once \n",
        "#print(IPFS_ROOT)\n",
        "print(experiment_path_suffix)\n",
        "#!echo /ipns/pollinations.ai/$experiment_path_suffix | DEBUG=* pollinate -p $experiment_path  -r --once\n",
        "!echo /ipns/pollinations.ai/ | DEBUG=* pollinate -p $IPFS_ROOT  -r --once\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydlP3in0PG3B"
      },
      "source": [
        "#!mkdir -p /content/ipfs/tiergarten/frames_8fps/valence_ourasl\n",
        "#!ffmpeg -i /content/ipfs/tiergarten-long.mp4 -r 8 /content/ipfs/tiergarten/frames_8fps/frame_%04d.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JarO2sCz7qlY"
      },
      "source": [
        "#!ffmpeg -i \"/content/ipfs/tiergarten/frames_8fps/\"%*.png /content/valenceArousel.mp4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpdJkdBssk9"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVr18E5tse8"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Downloading the model\n",
        "\n",
        "CLIP models are distributed as TorchScript modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFS29hnhlY4"
      },
      "source": [
        "MODELS = {\n",
        "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
        "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cboKZocQlSYX"
      },
      "source": [
        "! wget {MODELS[\"ViT-B/32\"]} -O model.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBRVTY9lbGm8"
      },
      "source": [
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer. The tokenizer code is hidden in the second cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGom156-i2kL"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqxNGd135sWC"
      },
      "source": [
        "print(experiment_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSSrLY185jSf"
      },
      "source": [
        "\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "images = []\n",
        "#texts = [\n",
        "plt.figure(figsize=(16, 16))\n",
        "images_path =  experiment_path+\"/frames\"\n",
        "\n",
        "filenames = os.listdir(images_path)\n",
        "filenames.sort()\n",
        "cols = 6\n",
        "filenames = [filename for filename in filenames if filename.endswith(\".png\") or filename.endswith(\".jpg\")]\n",
        "print(\"Preprocessing images...\", len(filenames))\n",
        "for filename in tqdm_notebook(filenames):\n",
        "    i = len(images)\n",
        "    #prop = i / len(images)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "\n",
        "    try :\n",
        "      image = preprocess(Image.open(os.path.join(images_path, filename)).convert(\"RGB\"))\n",
        "      images.append(image)\n",
        "    \n",
        "      #if i < cols*cols:\n",
        "        \n",
        "      #print(name)\n",
        "      if i<cols*cols // 2:\n",
        "        plt.subplot(6, 6, i+1)\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(f\"{filename}\\n\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        plt.tight_layout()\n",
        "      \n",
        "    except :\n",
        "      print(\"Error reading image\", name, \"Deleting\")\n",
        "      !rm -v $filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zugh3fF6CQP-"
      },
      "source": [
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feEA8lc78rYv"
      },
      "source": [
        "if cifar:\n",
        "  from torchvision.datasets import CIFAR100\n",
        "  words = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True).classes\n",
        "\n",
        "text_descriptions = [f\"This photo shows a {label.lower()}\" for label in words]\n",
        "\n",
        "print(\"\\n\".join(text_descriptions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwkkczUPBRMh"
      },
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "image_input -= image_mean[:, None, None]\n",
        "image_input /= image_std[:, None, None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBgCanxi8JKw"
      },
      "source": [
        "tokenizer = SimpleTokenizer()\n",
        "text_tokens = [tokenizer.encode(desc) for desc in text_descriptions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1l_muuhZ_Nk"
      },
      "source": [
        "text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "sot_token = tokenizer.encoder['<|startoftext|>']\n",
        "eot_token = tokenizer.encoder['<|endoftext|>']\n",
        "\n",
        "for i, tokens in enumerate(text_tokens):\n",
        "    tokens = [sot_token] + tokens + [eot_token]\n",
        "    text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "text_input = text_input.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN9I0nIBZ_vW"
      },
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    #text_features = model.encode_text(text_input).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxm2Gt4Wvzt"
      },
      "source": [
        "## Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "# Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o5Zpmx9bVWq"
      },
      "source": [
        "def get_probs(text_input):\n",
        "  with torch.no_grad():\n",
        "      text_features = model.encode_text(text_input).float()\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "  return text_probs\n",
        "  \n",
        "text_probs = get_probs(text_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_9AIZD3apd_"
      },
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def moving_average(x, w):\n",
        "    return ndimage.gaussian_filter(x, sigma=(50,1))\n",
        "    #return ndimage.convolve(x, np.ones((w,1))) / w\n",
        "\n",
        "def smooth_text_probs(text_probs):\n",
        "  text_probs_cpu = text_probs\n",
        "  #print(moving_average(text_probs_cpu,5).shape)\n",
        "\n",
        "  text_probs_smoothed = moving_average(text_probs,80)\n",
        "  # a Gaussian filter with a standard deviation of 10\n",
        "  #gauss = ndimage.gaussian_filter1d(img, 10, 1)\n",
        "  #text_probs_smoothed[0].shape\n",
        "\n",
        "  return text_probs_smoothed\n",
        "\n",
        "# mean over whole sequence\n",
        "#_, top_labels = torch.from_numpy(text_probs.cpu().numpy().mean(0)).cpu().topk(top_classes, dim=-1,sorted=False)\n",
        "# top_probs = text_probs[:,top_labels].cpu().numpy()\n",
        "\n",
        "# per frame\n",
        "text_probs = smooth_text_probs(text_probs.cpu())\n",
        "top_probs, top_labels = torch.from_numpy(text_probs).cpu().topk(top_classes, dim=-1,sorted=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpLX_0CzmKLs"
      },
      "source": [
        "#top_probs = (top_probs / 2) + ((top_probs.cpu().numpy() / np.amax(top_probs.cpu().numpy(), axis=0))/2)\n",
        "#top_probs =  ((top_probs.cpu().numpy() / np.amax(top_probs.cpu().numpy(), axis=0)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSI2Yx-ccc5F"
      },
      "source": [
        "\n",
        "top_probs.shape, top_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soIUhv4ta908"
      },
      "source": [
        "\n",
        "#top_probs, top_labels = text_probs_top.cpu().topk(top_classes, dim=-1,sorted=False)\n",
        "#top_probs = smooth_text_probs(top_probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJSoDL3GToum"
      },
      "source": [
        "import json\n",
        "print(results_path)\n",
        "resort_indices = np.argsort(top_labels.cpu().numpy())\n",
        "top_labels_sorted = np. take_along_axis(top_labels, resort_indices,axis=1)\n",
        "top_probs_sorted = np.take_along_axis(top_probs, resort_indices,axis=1)\n",
        "\n",
        "with open(f\"{results_path}/probabilities.json\",\"w\") as f:\n",
        "  json.dump({\n",
        "      \"words\": words,\n",
        "      \"probabilities\": top_probs_sorted.tolist()\n",
        "  }, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhftBGZp2Ax_"
      },
      "source": [
        "germanwords=[\"Waldschnepfe\",\"Mandarinenente\",\"Waschbär\",\"Flusskrebs\",\"Biber\"]\n",
        "\n",
        "plt.clf()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.title(\"Tiergarten Video\")\n",
        "plt.xlabel(\"Zeit (ms)\")\n",
        "plt.ylabel(\"Wahrscheinlichkeit\")\n",
        "plt.plot(top_probs_sorted[:,:10])\n",
        "\n",
        "plt.legend(germanwords)\n",
        "#plt.plot(text_probs_smoothed[:,:4])\n",
        "plt.show()\n",
        "#print(results_path)\n",
        "#print(top_labels[:100,:5])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBFRQmZfm8ft"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZIZKmalnG0T"
      },
      "source": [
        "\n",
        "from IPython.display import clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(len(images))\n",
        "image_no = 0\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "#!rm -rv $results_path\n",
        "!mkdir -p $results_path\n",
        "#!mkdir -p /content/ipfs/tiergarten/frames_8fps/valence_arousal/\n",
        "def plot_img(index):\n",
        "  #clear_output()\n",
        "  #plt.ioff()\n",
        "  image = images[index]\n",
        "  top_label = top_labels[index]\n",
        "  top_prop = top_probs[index]\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  img = image.permute(1, 2, 0)\n",
        "  plt.imshow(img)\n",
        " \n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(1, 2, 2)\n",
        "  y = np.arange(top_probs.shape[-1])\n",
        "  plt.grid()\n",
        "  plt.xlim([0,1])\n",
        "  plt.barh(y, top_probs[index])  \n",
        "  plt.gca().invert_yaxis()\n",
        "  plt.gca().set_axisbelow(True)\n",
        "  plt.yticks(y, [words[index] for index in top_label.numpy()])\n",
        "  plt.xlabel(\"probability\")\n",
        " \n",
        "  #plt.subplots_adjust(wspace=0.5)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"{results_path}/{experiment_name}_{'{:04}'.format(index)}.png\")\n",
        "  plt.close()\n",
        "  #plt.show()\n",
        " \n",
        "\n",
        "for i, image in enumerate(tqdm_notebook(images)):\n",
        "    print(\"frame:\",i)\n",
        "    plot_img(i)\n",
        " \n",
        " \n",
        "# plt.subplots_adjust(wspace=0.5)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCC215jOiAYl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoBoeTw1-Tl9"
      },
      "source": [
        "video_path = results_path+\"/\"+experiment_name+\".mp4\"\n",
        "print(\"video path\",video_path)\n",
        "!ffmpeg -i $results_path/%*.png  -vf format=yuv420p -r 10 -y $video_path\n",
        "!mv $results_path/*.png /tmp\n",
        "from google.colab import files\n",
        "files.download(video_path) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Ju_6IBE2Iz"
      },
      "source": [
        "blasdas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OENu-DQLzQY"
      },
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "\n",
        "fps = 30\n",
        "nSeconds = 5\n",
        "snapshots = [ np.random.rand(5,5) for _ in range( nSeconds * fps ) ]\n",
        "\n",
        "# First set up the figure, the axis, and the plot element we want to animate\n",
        "fig = plt.figure( figsize=(8,8) )\n",
        "\n",
        "a = snapshots[0]\n",
        "im = plt.imshow(a, interpolation='none', aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "def animate_func(i):\n",
        "    if i % fps == 0:\n",
        "        print( '.', end ='' )\n",
        "\n",
        "    im.set_array(snapshots[i])\n",
        "    return [im]\n",
        "\n",
        "anim = animation.FuncAnimation(\n",
        "                               fig, \n",
        "                               animate_func, \n",
        "                               frames = nSeconds * fps,\n",
        "                               interval = 1000 / fps, # in ms\n",
        "                               )\n",
        "\n",
        "anim.save('test_anim.mp4', fps=fps, extra_args=['-vcodec', 'libx264'])\n",
        "\n",
        "print('Done!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qyHOXC5ACE4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}